{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "After 0 training step(s), validation accuracy using average model is 0.1304,                test accuracy using average model is 0.1319\n",
      "After 30 training step(s), test accuracy using         average model is 0.1319 \n"
     ]
    }
   ],
   "source": [
    "# 滑动平均模型\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 一个训练batch中的训练数据个数。数字越小时，训练过程越接近随机梯度下降；数字越大时，训练越接近梯度下降\n",
    "BATCH_SIZE=100\n",
    "\n",
    "# 输入层的节点数，对于MNIST数据集，这个就等于图片的像素\n",
    "INPUT_NODE=784  \n",
    "\n",
    "# 隐藏层节点数，这里使用只有一个隐藏层的网络结构作为样例，这个隐藏层有500个节点\n",
    "LAYER1_NODE=500\n",
    "\n",
    "#输出层的节点数，这个等于类别的数目，因为在MNIST数据集中需要区分的是0-9这10个数字，所以这里输出层的节点数为10\n",
    "OUTPUT_NODE=10\n",
    "\n",
    "# 基础的学习率\n",
    "LEARNING_RATE_BASE=0.8\n",
    "\n",
    "# 训练轮数\n",
    "TRAINING_STEPS=30\n",
    "\n",
    "# 滑动平均衰减率\n",
    "MOVING_VERAGE_DECAY=0.99\n",
    "\n",
    "# 一个辅助函数，给定神经网络的输入和所有参数，计算神经网络的向前传播结果，这里定义了一个使用RELU激活函数的三层全连接神经网络\n",
    "# 通过加入隐藏层实现了多层网络结构，通过RELU函数实现了去线性化，在这个函数中也支持传入用于计算参数平均值的类，\n",
    "# 这样方便在测试时使用滑动平均模型\n",
    "\n",
    "def inference(input_tensor,avgclass,weights1,biases1,weights2,biases2):\n",
    "#     当没有提供滑动平均参类时，直接使用参数当前的取值\n",
    "    if avgclass == None:\n",
    "#         记录隐藏层的前向传播结果，这里使用了RELU激活函数\n",
    "        layer1=tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)\n",
    "#         计算输出层的前向传播结果，因为在计算损失函数时会一并计算softmax函数\n",
    "#         所以这里不需要加入激活函数。而且不加入softmax不会影响预测结果。\n",
    "#         因为与测试使用的是不同类别对应节点输出值的相对大小，有没有softmax\n",
    "#         层对最后分类结果的计算没有影响。于是在计算整个神经网络的前向传播时可以不加入最后的softmax层\n",
    "        return tf.matmul(layer1,weights2)+biases2\n",
    "\n",
    "    else:\n",
    "#         首先使用avg_class.average函数来计算得出变量的平均滑动值\n",
    "#         然后在计算相应的神经网络前向传播结果\n",
    "        layer1=tf.nn.relu(tf.matmul(input_tensor,avgclass.average(weights1))+avgclass.average(biases1))\n",
    "        return tf.matmul(layer1,avgclass.average(weights2))+avgclass.average(biases2)\n",
    "# 训练模型的过程\n",
    "def train(mnist):\n",
    "    x = tf.placeholder(tf.float32,[None,INPUT_NODE],name='x_input')\n",
    "    y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name='y_input')\n",
    "    \n",
    "#     生成隐藏层的参数\n",
    "    weight1=tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER1_NODE],stddev=1.0))\n",
    "    biases1=tf.Variable(tf.constant(0.1,shape=[LAYER1_NODE]))\n",
    "    \n",
    "#     生成输出层的参数\n",
    "    weight2=tf.Variable(tf.truncated_normal([LAYER1_NODE,OUTPUT_NODE],stddev=0.1))\n",
    "    biases2=tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))\n",
    "    \n",
    "#     计算在当前参数下神经网络前向传播的结果，这里给出的用于计算滑动平均的类为None\n",
    "#     所以函数不会使用参数的滑动平均值\n",
    "    y=inference(x,None,weight1,biases1,weight2,biases2)\n",
    "    \n",
    "#     定义存储训练论述的变量，这个变量不需要计算滑动平均值，所以指定这个变量为不可恩训练的变量（trainable=False）\n",
    "#     在使用tensorflow训练神经网络时，一般会将代表训练论述的变量指定为不可训练的参数\n",
    "    global_step=tf.Variable(0,trainable=False)\n",
    "    \n",
    "#     给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类。给定训练轮数的变量可以加快训练早期变量的更新速度\n",
    "    variable_average=tf.train.ExponentialMovingAverage(MOVING_VERAGE_DECAY,global_step)\n",
    "    \n",
    "#     在所有代表神经网络参数的变量上使用滑动平均，其他辅助变量如global_step就不需要了。tf.trainable_variables\n",
    "#     返回的就是图上集合 GraphKeys.TRAINABLE_VARIABLES中的元素。这个集合的元素就是所有没有指定trainable=False的参数\n",
    "    variable_average_op=variable_average.apply(tf.trainable_variables())\n",
    "    \n",
    "#     计算使用了滑动平均之后的前向传播效果，滑动平均不会改变变量本身的取值，而是维护一个影子变量来记录其滑动平均\n",
    "#     取值，所以当需要去哦使用这个滑动平均值时，需要明确调用average函数\n",
    "    average_y = inference(x,variable_average,weight1,biases1,weight2,biases2)\n",
    "    \n",
    "#     计算交叉熵作为刻画预测值与真实值之间差距的损失函数，这里使用了sparse_softmax_entropy_with_logits函数来\n",
    "#     计算交叉熵。当分类问题只有一个正确答案时，可以使用这个函数来加速交叉熵的计算，MNIST问题的图片中\n",
    "#     只包含了0-9中的一个数字，所以可以使用这个函数来计算交叉熵的损失。这个函数的第一个参数是神经网咯不包括softax层\n",
    "#     的前向传播的结果,第二个参数是训练数据的证据答案.因为标准答案是一个长度为10的一维数组,而该函数需要提供的是一个\n",
    "#     正确答案的数组,所以需要使用tf.argmax函数来达到整个却答案对应的类别编号\n",
    "    cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))       \n",
    "    train_step=tf.train.GradientDescentOptimizer(LEARNING_RATE_BASE).minimize(cross_entropy,global_step=global_step)\n",
    "    correction_prediction=tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "\n",
    "#这个运算首先将一个布尔型的数值转换为实数型，然后计算平均值。这个平均值就是模型在这一组数据上的正确率\n",
    "    accuracy=tf.reduce_mean(tf.cast(correction_prediction,tf.float32))\n",
    "    # 初始化会话并开始训练过程\n",
    "    with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "        #准备验证数据。一般在神经网络的训练过程中会通过验证数据来大致判断停止的\n",
    "            #条件和评判训练的效果\n",
    "        validate_feed={x:mnist.validation.images,y_:mnist.validation.labels}\n",
    "        #准备测试数据。在真实的应用中，这部分数据在训练时是不可见的，这个数据只是作为\n",
    "            #模型优劣的最后评判标准\n",
    "        test_feed = {x:mnist.test.images,y_:mnist.test.labels}\n",
    "\n",
    "        #迭代地训练神经网络\n",
    "        for i in range(TRAINING_STEPS):\n",
    "            #每1000轮输出一次在验证数据集上的测试结果\n",
    "            if i%1000==0:\n",
    "                #将模型保存到这个文件下\n",
    "#                 save_path = saver.save(sess, 'model/mnist_model.ckpt')\n",
    "\n",
    "                #计算滑动平均模型在验证数据上的结果。因为MNIST数据集比较小，所以一次\n",
    "                #可以处理所有的验证数据。为了计算方便，本样例程序没有将验证数据划分为更小的batch\n",
    "                #当神经网络模型比较复杂或者验证数据比较大时，太大的batch\n",
    "                #会导致计算时间过长甚至发生内存溢出的错误\n",
    "                validate_acc = sess.run(accuracy,feed_dict=validate_feed)\n",
    "                test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "                print(\"After %d training step(s), validation accuracy using average model is %g,\\\n",
    "                test accuracy using average model is %g\" %(i,validate_acc,test_acc))\n",
    "                #产生这一轮使用的一个batch的训练数据，并运行训练过程\n",
    "            xs,ys=mnist.train.next_batch(BATCH_SIZE)\n",
    "#             sess.run(train_op,feed_dict={x:xs,y_:ys})\n",
    "        #在训练结束之后，在测试数据上检测神经网络模型的最终正确率\n",
    "        test_acc=sess.run(accuracy,feed_dict=test_feed)\n",
    "        print(\"After %d training step(s), test accuracy using \\\n",
    "        average model is %g \"%(TRAINING_STEPS,test_acc))\n",
    "\n",
    "#主程序入口\n",
    "def main(argv=None):\n",
    "    #声明处理MNIST数据集的类，这个类在初始化时会自动下载数据\n",
    "    mnist=input_data.read_data_sets(\"MNIST_data/\",one_hot=True)\n",
    "    train(mnist)     \n",
    " \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Failed to convert object of type <class 'method'> to Tensor. Contents: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x000000E039CD2DA0>>. Consider casting elements to a supported type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m       \u001b[0mstr_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m       \u001b[0mstr_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mproto_values\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\compat.py\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     67\u001b[0m     raise TypeError('Expected binary or unicode string, got %r' %\n\u001b[1;32m---> 68\u001b[1;33m                     (bytes_or_text,))\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x000000E039CD2DA0>>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-a2faed4ca544>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;31m#     计算使用了滑动平均之后的前向传播效果，滑动平均不会改变变量本身的取值，而是维护一个影子变量来记录其滑动平均\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m#     取值，所以当需要去哦使用这个滑动平均值时，需要明确调用average函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m \u001b[0maverage_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvariable_average\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[0mcross_entropy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconv_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-a2faed4ca544>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(variable_average)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mweight_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetWeight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0mbias_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetBias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mconv_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvariable_average\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbias_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mpool_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-a2faed4ca544>\u001b[0m in \u001b[0;36minference\u001b[1;34m(input_tensor, avg_class, weight, bias)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavg_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mavg_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#     专门为全卷积\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-a2faed4ca544>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(x, w)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m      \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmax_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name)\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;34m\"Conv2D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m         data_format=data_format, dilations=dilations, name=name)\n\u001b[0m\u001b[0;32m    953\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 513\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m               raise TypeError(\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    508\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    511\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1039\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    233\u001b[0m                                          as_ref=False):\n\u001b[0;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[0;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[0;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[1;32m--> 214\u001b[1;33m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[0;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[1;32mC:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[1;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    520\u001b[0m       raise TypeError(\"Failed to convert object of type %s to Tensor. \"\n\u001b[0;32m    521\u001b[0m                       \u001b[1;34m\"Contents: %s. Consider casting elements to a \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m                       \"supported type.\" % (type(values), values))\n\u001b[0m\u001b[0;32m    523\u001b[0m     \u001b[0mtensor_proto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor_proto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Failed to convert object of type <class 'method'> to Tensor. Contents: <bound method Kernel.raw_input of <ipykernel.ipkernel.IPythonKernel object at 0x000000E039CD2DA0>>. Consider casting elements to a supported type."
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('MNIST_data/',one_hot=True)\n",
    "\n",
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y_=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "x_image=tf.reshape(x,[-1,28,28,1])\n",
    "sess=tf.InteractiveSession()\n",
    "def getWeight(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=1.0,mean=1.0)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def getBias(shape):\n",
    "    initial=tf.constant(0.1,shape=shape)\n",
    "    return initial\n",
    "    \n",
    "def conv2d(x,w):\n",
    "     return tf.nn.conv2d(x,w,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "# 卷积层\n",
    "def inference(input_tensor,avg_class,weight,bias):\n",
    "    if avg_class == None:\n",
    "        return conv2d(input_tensor,weight)+bias\n",
    "    else:\n",
    "        return conv2d(input,avg_class.average(weight))+avg_class.average(bias)\n",
    "\n",
    "#     专门为全卷积\n",
    "def fcn_inference(input_tensor,avg_class,weight,bias):\n",
    "    if avg_class:\n",
    "        return tf.matmul(input_tensor,avg_class.average(weight))+avg_class.average(bias)\n",
    "    else:\n",
    "        return tf.matmul(input_tensor,weight)+bias\n",
    "\n",
    "\n",
    "def train(variable_average=None):\n",
    "    if variable_average:\n",
    "            # 第一层\n",
    "        weight_1=getWeight([5,5,1,32])\n",
    "        bias_1=getBias([32])\n",
    "        conv_1=inference(x_image,variable_average,weight_1,bias_1)\n",
    "        pool_1=max_pool(conv_1)\n",
    "\n",
    "        # 第二层\n",
    "        weight_2=getWeight([3,3,32,64])\n",
    "        bias_2=getBias([64])\n",
    "        conv_2=inference(pool_1,variable_average,weight_2,bias_2)\n",
    "        pool_2=max_pool(conv_2)\n",
    "\n",
    "        # 第三层\n",
    "        weight_3=getWeight([7*7*64,10])\n",
    "        bias_3=getBias([10])\n",
    "        pool_2=tf.reshape(pool_2,[-1,7*7*64])\n",
    "        conv_3=fcn_inference(pool_2,variable_average,weight_3,bias_3)\n",
    "    else:\n",
    "        # 第一层\n",
    "        weight_1=getWeight([5,5,1,32])\n",
    "        bias_1=getBias([32])\n",
    "        conv_1=inference(x_image,None,weight_1,bias_1)\n",
    "        pool_1=max_pool(conv_1)\n",
    "\n",
    "        # 第二层\n",
    "        weight_2=getWeight([7,7,32,64])\n",
    "        bias_2=getBias([64])\n",
    "        conv_2=inference(pool_1,None,weight_2,bias_2)\n",
    "        pool_2=max_pool(conv_2)\n",
    "\n",
    "        # 第三层\n",
    "        weight_3=getWeight([7*7*64,10])\n",
    "        bias_3=getBias([10])\n",
    "        pool_2=tf.reshape(pool_2,[-1,7*7*64])\n",
    "        conv_3=fcn_inference(pool_2,None,weight_3,bias_3)\n",
    "    return conv_3\n",
    "\n",
    "conv_3=train()\n",
    "#     定义存储训练论述的变量，这个变量不需要计算滑动平均值，所以指定这个变量为不可恩训练的变量（trainable=False）\n",
    "#     在使用tensorflow训练神经网络时，一般会将代表训练论述的变量指定为不可训练的参数\n",
    "global_step=tf.Variable(0,trainable=False)\n",
    "\n",
    "#     给定滑动平均衰减率和训练轮数的变量，初始化滑动平均类。给定训练轮数的变量可以加快训练早期变量的更新速度\n",
    "variable_average=tf.train.ExponentialMovingAverage(MOVING_VERAGE_DECAY,global_step)\n",
    "\n",
    "#     在所有代表神经网络参数的变量上使用滑动平均，其他辅助变量如global_step就不需要了。tf.trainable_variables\n",
    "#     返回的就是图上集合 GraphKeys.TRAINABLE_VARIABLES中的元素。这个集合的元素就是所有没有指定trainable=False的参数\n",
    "variable_average_op=variable_average.apply(tf.trainable_variables())\n",
    "\n",
    "#     计算使用了滑动平均之后的前向传播效果，滑动平均不会改变变量本身的取值，而是维护一个影子变量来记录其滑动平均\n",
    "#     取值，所以当需要去哦使用这个滑动平均值时，需要明确调用average函数\n",
    "average_y = train(variable_average)\n",
    "\n",
    "cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=conv_3,labels=y_)\n",
    "train_step=tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy,global_step=global_step)\n",
    "# correction_prediction=tf.equal(tf.argmax(conv_3,1),tf.argmax(y_,1))\n",
    "correction_prediction=tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correction_prediction,tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "steps=5\n",
    "for i in range(steps):\n",
    "    batch=mnist.train.next_batch(batch_size)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy=accuracy.eval(feed_dict={x:batch[0],y_:batch[1]})\n",
    "        print(\"step %d,train accuracy %g\" %(i,train_accuracy))\n",
    "        train_step.run(feed_dict={x:batch[0],y_:batch[1]})\n",
    "        \n",
    "print(\"test accuracy %g\" %accuracy.eval(feed_dict={x:mnist.test.images,y_:mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-1-4ac03434ff03>:8: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Program Files (x86)\\software\\Anaconda\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From <ipython-input-1-4ac03434ff03>:99: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "0: accuracy:0.05 loss: 302.39838 (lr:0.02)\n",
      "0: ********* epoch 1 ********* test accuracy:0.0876 test loss: 2087615900000.0\n",
      "1: accuracy:0.58 loss: 143.00208 (lr:0.019987566385909143)\n",
      "1: ********* epoch 1 ********* test accuracy:0.5116 test loss: 181.53365\n",
      "2: accuracy:0.66 loss: 102.116 (lr:0.019975140540399162)\n",
      "2: ********* epoch 1 ********* test accuracy:0.6432 test loss: 116.83943\n",
      "3: accuracy:0.66 loss: 80.05922 (lr:0.019962722458616203)\n",
      "3: ********* epoch 1 ********* test accuracy:0.7668 test loss: 76.824104\n",
      "4: accuracy:0.87 loss: 42.30413 (lr:0.019950312135709455)\n",
      "4: ********* epoch 1 ********* test accuracy:0.8409 test loss: 56.08923\n",
      "5: accuracy:0.86 loss: 42.917713 (lr:0.01993790956683114)\n",
      "5: ********* epoch 1 ********* test accuracy:0.8318 test loss: 58.220936\n",
      "6: accuracy:0.82 loss: 73.62039 (lr:0.019925514747136504)\n",
      "6: ********* epoch 1 ********* test accuracy:0.8412 test loss: 54.872185\n",
      "7: accuracy:0.81 loss: 57.4712 (lr:0.019913127671783815)\n",
      "7: ********* epoch 1 ********* test accuracy:0.8591 test loss: 48.003624\n",
      "8: accuracy:0.83 loss: 46.895123 (lr:0.019900748335934377)\n",
      "8: ********* epoch 1 ********* test accuracy:0.8741 test loss: 43.082935\n",
      "9: accuracy:0.88 loss: 31.978039 (lr:0.01988837673475251)\n",
      "9: ********* epoch 1 ********* test accuracy:0.8884 test loss: 40.912437\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import math  \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "tf.set_random_seed(0.0)  \n",
    "  \n",
    "# Download images and labels into mnist.test (10K images+labels)   \n",
    "# and mnist.train (60K images+labels)  \n",
    "mnist=input_data.read_data_sets('MNIST_data/',one_hot=True)\n",
    "  \n",
    "# Ylogits      - input data that need tobe batch normalised. For convolutional  \n",
    "#                layer, it's a 4-D tensor. For fully connected layer, it's a 2-D tensor  \n",
    "# is_test      - flag, is_test = False for train  \n",
    "#                      is_test = True  for test  \n",
    "# offset       - beta  \n",
    "#                gamma(scaling) is not useful for relu   \n",
    "def batchnormForRelu(Ylogits, is_test, Iteration, offset, convolutional=False):  \n",
    "    # adding the iteration prevents from averaging across non-existing iterations  \n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, Iteration)   \n",
    "    bnepsilon = 1e-5  \n",
    "    if convolutional:  \n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])  \n",
    "    else:  \n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])  \n",
    "    update_moving_averages = exp_moving_avg.apply([mean, variance])  \n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)  \n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)  \n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)  \n",
    "    return Ybn, update_moving_averages  \n",
    "  \n",
    "def compatible_convolutional_noise_shape(Y):  \n",
    "    noiseshape = tf.shape(Y)  \n",
    "    noiseshape = noiseshape * tf.constant([1,0,0,1]) + tf.constant([0,1,1,0])  \n",
    "    return noiseshape  \n",
    "  \n",
    "# input X: 28x28 grayscale images  \n",
    "X  = tf.placeholder(tf.float32, [None,784]) \n",
    "X_image=tf.reshape(X,[-1,28,28,1])\n",
    "# correct answers will go here  \n",
    "Y_ = tf.placeholder(tf.float32, [None, 10])  \n",
    "# variable learning rate  \n",
    "lr = tf.placeholder(tf.float32)  \n",
    "# test flag for batch norm  \n",
    "tst = tf.placeholder(tf.bool)  \n",
    "Iter = tf.placeholder(tf.int32)  \n",
    "# dropout probability  \n",
    "pkeep = tf.placeholder(tf.float32)  \n",
    "pkeep_conv = tf.placeholder(tf.float32)  \n",
    "  \n",
    "# three convolutional layers with their channel counts, and a  \n",
    "# fully connected layer (tha last layer has 10 softmax neurons)  \n",
    "K = 24  # 1st convolutional layer output depth  \n",
    "L = 48  # 2nd convolutional layer output depth  \n",
    "M = 64  # 3rd convolutional layer  \n",
    "N = 200 # 4th fully connected layer  \n",
    "  \n",
    "W1 = tf.Variable(tf.truncated_normal([6, 6, 1, K], stddev=0.1))  # 6x6 patch, 1 input channel, K output channels  \n",
    "B1 = tf.Variable(tf.constant(0.1, tf.float32, [K]))  \n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))  \n",
    "B2 = tf.Variable(tf.constant(0.1, tf.float32, [L]))  \n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.1))  \n",
    "B3 = tf.Variable(tf.constant(0.1, tf.float32, [M]))  \n",
    "  \n",
    "W4 = tf.Variable(tf.truncated_normal([7 * 7 * M, N], stddev=0.1))  \n",
    "B4 = tf.Variable(tf.constant(0.1, tf.float32, [N]))  \n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))  \n",
    "B5 = tf.Variable(tf.constant(0.1, tf.float32, [10]))  \n",
    "  \n",
    "# The model  \n",
    "# batch norm scaling is not useful with relus  \n",
    "# batch norm offsets are used instead of biases  \n",
    "Y1l = tf.nn.conv2d(X_image, W1, strides=[1, 1, 1, 1], padding='SAME')  \n",
    "Y1bn, update_ema1 = batchnormForRelu(Y1l, tst, Iter, B1, convolutional=True)  \n",
    "Y1r = tf.nn.relu(Y1bn)  \n",
    "Y1 = tf.nn.dropout(Y1r, pkeep_conv, compatible_convolutional_noise_shape(Y1r))  \n",
    "stride = 2  # output is 14x14  \n",
    "Y2l = tf.nn.conv2d(Y1, W2, strides=[1, stride, stride, 1], padding='SAME')  \n",
    "Y2bn, update_ema2 = batchnormForRelu(Y2l, tst, Iter, B2, convolutional=True)  \n",
    "Y2r = tf.nn.relu(Y2bn)  \n",
    "Y2 = tf.nn.dropout(Y2r, pkeep_conv, compatible_convolutional_noise_shape(Y2r))  \n",
    "stride = 2  # output is 7x7  \n",
    "Y3l = tf.nn.conv2d(Y2, W3, strides=[1, stride, stride, 1], padding='SAME')  \n",
    "Y3bn, update_ema3 = batchnormForRelu(Y3l, tst, Iter, B3, convolutional=True)  \n",
    "Y3r = tf.nn.relu(Y3bn)  \n",
    "Y3 = tf.nn.dropout(Y3r, pkeep_conv, compatible_convolutional_noise_shape(Y3r))  \n",
    "# reshape  \n",
    "YY = tf.reshape(Y3, shape=[-1, 7 * 7 * M])  \n",
    "Y4l = tf.matmul(YY, W4)  \n",
    "Y4bn, update_ema4 = batchnormForRelu(Y4l, tst, Iter, B4)  \n",
    "Y4r = tf.nn.relu(Y4bn)  \n",
    "Y4 = tf.nn.dropout(Y4r, pkeep)  \n",
    "Ylogits = tf.matmul(Y4, W5) + B5  \n",
    "Y = tf.nn.softmax(Ylogits)  \n",
    "  \n",
    "update_ema = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)  \n",
    "  \n",
    "# cross-entropy loss function (= -sum(Y_i * log(Yi)) ), normalised for batches of 100  images  \n",
    "# TensorFlow provides the softmax_cross_entropy_with_logits function to avoid numerical stability  \n",
    "# problems with log(0) which is NaN  \n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Y_)  \n",
    "cross_entropy = tf.reduce_mean(cross_entropy)*100  \n",
    "  \n",
    "# accuracy of the trained model, between 0 (worst) and 1 (best)  \n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))  \n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  \n",
    "  \n",
    "# training step, the learning rate is a placeholder  \n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(cross_entropy)  \n",
    "  \n",
    "# init  \n",
    "init = tf.global_variables_initializer()  \n",
    "sess = tf.Session()  \n",
    "sess.run(init)  \n",
    "  \n",
    "def training_step(i, update_test_data, update_train_data):  \n",
    "  \n",
    "    # training on batches of 100 images with 100 labels  \n",
    "    batch_X, batch_Y = mnist.train.next_batch(100)  \n",
    "  \n",
    "    # learning rate decay  \n",
    "    max_learning_rate = 0.02  \n",
    "    min_learning_rate = 0.0001  \n",
    "    decay_speed = 1600  \n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-i/decay_speed)  \n",
    "  \n",
    "    # compute training values for visualisation  \n",
    "    if update_train_data:  \n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        {X: batch_X, Y_: batch_Y, tst: False, pkeep: 1.0, pkeep_conv: 1.0})  \n",
    "        print(str(i) + \": accuracy:\" + str(a) + \" loss: \" + str(c) + \" (lr:\" + str(learning_rate) + \")\")  \n",
    "    # compute test values for visualisation  \n",
    "    if update_test_data:  \n",
    "        a, c = sess.run([accuracy, cross_entropy],\n",
    "                        {X: mnist.test.images, Y_: mnist.test.labels, tst: True, pkeep: 1.0, pkeep_conv: 1.0})  \n",
    "        print(str(i) + \": ********* epoch \" + str(i*100//mnist.train.images.shape[0]+1) + \" ********* test accuracy:\" + str(a) + \" test loss: \" + str(c))  \n",
    "    # the backpropagation training step  \n",
    "    sess.run(train_step, {X: batch_X, Y_: batch_Y, lr: learning_rate, tst: False, pkeep: 0.75, pkeep_conv: 1.0})  \n",
    "    sess.run(update_ema, {X: batch_X, Y_: batch_Y, tst: False, Iter: i, pkeep: 1.0, pkeep_conv: 1.0})  \n",
    "  \n",
    "if __name__ == \"__main__\":  \n",
    "    for i in range(0, 10):  \n",
    "       training_step(i, True, True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
