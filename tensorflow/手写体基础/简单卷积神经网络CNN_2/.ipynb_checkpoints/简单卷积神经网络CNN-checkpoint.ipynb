{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784) (55000, 10)\n",
      "(10000, 784) (10000, 10)\n",
      "(5000, 784) (5000, 10)\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('../MNIST_data/',one_hot=True)\n",
    "sess=tf.InteractiveSession()\n",
    "\n",
    "print(mnist.train.images.shape,mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape,mnist.test.labels.shape)\n",
    "print(mnist.validation.images.shape,mnist.validation.labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial=tf.truncated_normal(shape,stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_Variable(shape):\n",
    "    initial=tf.constant(0.1,shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable_81/read:0\", shape=(5, 5, 1, 32), dtype=float32)\n",
      "Tensor(\"Variable_81/read:0\", shape=(5, 5, 1, 32), dtype=float32)\n",
      "Tensor(\"Relu_25:0\", shape=(?, 28, 28, 32), dtype=float32) h_conv1\n",
      "Tensor(\"MaxPool_22:0\", shape=(?, 14, 14, 32), dtype=float32) h_pool1\n",
      "sted 0,train accuracy 0.08\n",
      "test accuracy 0.0982\n"
     ]
    }
   ],
   "source": [
    "x=tf.placeholder(tf.float32,[None,784])\n",
    "y_=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "x_image=tf.reshape(x,[-1,28,28,1])\n",
    "W_conv1=weight_variable([5,5,1,32])\n",
    "print(W_conv1.value())\n",
    "\n",
    "# 第一层\n",
    "b_conv1=bias_Variable([5,5,1,32])\n",
    "print(W_conv1.value())\n",
    "b_conv1=bias_Variable([32])\n",
    "h_conv1=tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "print(h_conv1,'h_conv1')\n",
    "h_pool1=max_pool_2x2(h_conv1)\n",
    "print(h_pool1,'h_pool1')\n",
    "\n",
    "# 第二层\n",
    "W_conv2=weight_variable([3,3,32,64])\n",
    "b_conv2=bias_Variable([64])\n",
    "h_conv2=tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "h_pool2=max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "# 平铺\n",
    "W_fc1=weight_variable([7*7*64,1024])\n",
    "b_fc1=bias_Variable([1024])\n",
    "h_pool2_flat=tf.reshape(h_pool2,[-1,7*7*64])\n",
    "h_fc1=tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "\n",
    "\n",
    "keep_prob=tf.placeholder(tf.float32)\n",
    "h_fc1_drop=tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "#平铺2\n",
    "W_fc2=weight_variable([1024,10])\n",
    "b_fc2=bias_Variable([10])\n",
    "y_conv=tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "\n",
    "cross_entropy=tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv),reduction_indices=[1]))\n",
    "train_step=tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correction_prediction=tf.equal(tf.arg_max(y_conv,1),tf.arg_max(y_,1))\n",
    "accuracy=tf.reduce_mean(tf.cast(correction_prediction,tf.float32))\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for i in range(5):\n",
    "    batch=mnist.train.next_batch(50)\n",
    "    if i %100 ==0:\n",
    "        train_accuracy=accuracy.eval(feed_dict={x:batch[0],y_:batch[1],keep_prob:1.0})\n",
    "        print(\"sted %d,train accuracy %g\"%(i,train_accuracy))\n",
    "    train_step.run(feed_dict={x:batch[0],y_:batch[1],keep_prob:0.5})\n",
    "\n",
    "\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict = {x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.]\n",
      "   [2.]\n",
      "   [3.]]\n",
      "\n",
      "  [[4.]\n",
      "   [5.]\n",
      "   [6.]]]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[1., 2., 3.],\n",
    "                 [4., 5., 6.]])\n",
    "\n",
    "x = tf.reshape(x, [1, 2, 3, 1])\n",
    "print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_14:0\", shape=(1, 1, 1, 1), dtype=float32)\n",
      "[[[[5.]]]]\n",
      "Tensor(\"MaxPool_15:0\", shape=(1, 1, 2, 1), dtype=float32)\n",
      "[[[[5.]\n",
      "   [6.]]]]\n"
     ]
    }
   ],
   "source": [
    "valid_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "same_pad = tf.nn.max_pool(x, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "print(valid_pad)\n",
    "print(valid_pad.eval())\n",
    "print(same_pad)\n",
    "print(same_pad.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(55000, 784) (55000, 10)\n",
      "(10000, 784) (10000, 10)\n",
      "(5000, 784) (5000, 10)\n",
      "step 0,train accuracy 0.12\n",
      "test accuracy 0.1415\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "# 下载并导入mnist数据\n",
    "mnist = input_data.read_data_sets(\"../MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Tensorflow默认会话\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# 打印训练集，测试集，验证集的image和label的shape\n",
    "print(mnist.train.images.shape, mnist.train.labels.shape)\n",
    "print(mnist.test.images.shape, mnist.test.labels.shape)\n",
    "print(mnist.validation.images.shape, mnist.validation.labels.shape)\n",
    "\n",
    "# 定义权重变量的函数，参数是需要使用的权重的shape\n",
    "def weight_variable(shape):\n",
    "    # 使用正态分布填充变量，标准差设置为0.1\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    #返回相应的变量\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 定义一个返回全部由0.1组成的指定shape的tensor的函数\n",
    "def bias_Variable(shape):\n",
    "    # 定义一个常量该常量全部由0.1组成，shape由参数shape指定\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    # 返回一个变量，变量的值由刚刚定义的常量决定\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# 卷积函数，这里把Tensorflow提供的卷积函数做了一个小小的封装。\n",
    "# 这里默认strides是[1,1,1,1]，即只能将卷积核每次都移动一步。\n",
    "# 默认使用边缘填充0的策略（padding=\"SAME\"），这样保持卷积前后的矩阵在第一维第二维上是相等的，注意这里深度可能是不同的。\n",
    "# 参数x表示的是待卷积的矩阵，一般是一张图片。\n",
    "# 参数W表示的是卷积核的相关信息。W一般是一个由4个数字组成的list：[a,b,c,d]。\n",
    "# a,b,c参数限制的卷积核的结构，a，b表示的是平面上的大小，c表示深度，一般可以理解为图片的通道数目。\n",
    "# d表示的是卷积核的深度，可以理解为卷积核的数目，即你想要提取的特征的数目。\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1],padding=\"SAME\")\n",
    "\n",
    "# 池化函数，同卷积函数，这里也将池化函数做了一个小小的封装。\n",
    "# 这里x是待池化的矩阵。\n",
    "# ksize是池化核的大小，和卷积核一样，一般是一个四个数字组成的list，第一个数字和最后一个数字应为1。\n",
    "# strdides是步长参数，一般第一个数字和最后一个数字都是1。第一个1是tensor的维度表示1，最后一个1是指最里面的向量是1维的\n",
    "# padding参数和卷积核一样，这里使用\"SAME\"，表示使用边缘填充0的策略。\n",
    "# 池化操作一般不改变矩阵的深度，但是会改变矩阵的大小。\n",
    "# 这里采用的是最大池化，同时还有平均池化方法。\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1],padding=\"SAME\")\n",
    "\n",
    "# 定义传入数据的placeholder\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "# 因为CNN网络需要考虑像素的空间位置，因此需要把其空间位置还原。\n",
    "# 这里-1表示样本数量不固定，两个28表示空间尺寸，1表示1个通道数目。\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "# 获取卷积核，这里卷积核的大小是5x5,1个通道，32个卷积核，提取32个特征。\n",
    "W_conv1 = weight_variable([5,5,1,32])\n",
    "\n",
    "# 获取偏差值，偏差值得数目应该和卷积核的数目保持一致。\n",
    "b_conv1 = bias_Variable([32])\n",
    "\n",
    "# 将输入的图像做卷积操作，最有加上偏差值，使用relu函数去线性化。\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1)+b_conv1)\n",
    "\n",
    "# 对去线性化的矩阵进行池化操作。\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "# 类似的，重复上述操作，注意这里输入的矩阵的深度是32，与上一次卷积过程卷积核的数目相同。\n",
    "# 这里我们使用的卷积核的数目是64。\n",
    "W_conv2 = weight_variable([5,5,32,64])\n",
    "b_conv2 = bias_Variable([64])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2)+b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "# 下面定义一个全连接层。\n",
    "\n",
    "# 我们经过两次卷积和池化操作，最后图片的大小变为7x7，原因请参考池化操作时的池化核和池化步长。\n",
    "# 因为均为2x2，所以每次池化之后，大小均会长宽均会缩小至原来的一半。\n",
    "# 这里设置1024个隐藏节点用于保存参数。\n",
    "W_fc1 = weight_variable([7*7*64,1024])\n",
    "\n",
    "# 设置1024个偏置量。\n",
    "b_fc1 = bias_Variable([1024])\n",
    "\n",
    "# 将经过两次卷积池化操作之后的矩阵整理为一维向量，长度为7*7*64。\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,7*7*64])\n",
    "\n",
    "# 矩阵相乘并加上相关的偏置量，最后使用Relu函数去线性化。\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "# 定义一个保留的概率，这里是一个placeholder。\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# 使用dropout方法按照一定概率（keep_prob指定）使得矩阵的某些数据不起作用。\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "# 定义又一个全连接层。因为我们需要将结果映射到10个类别上，因此第二个参数是10。\n",
    "W_fc2 = weight_variable([1024,10])\n",
    "\n",
    "# 定义偏置量。\n",
    "b_fc2 = bias_Variable([10])\n",
    "\n",
    "# 矩阵相乘，并加上偏置量，随后使用softmax处理。\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)\n",
    "\n",
    "# 定义交叉熵函数，即我们使用的loss函数，我们的目的就是在训练过程中使得loss函数尽可能的小。\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_*tf.log(y_conv),reduction_indices=[1]))\n",
    "\n",
    "# 使用AdamOptimizer进行loss函数的优化。\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# 定义预测函数，这里比较预测结果和实际结果的相匹配的数量\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1),tf.argmax(y_,1))\n",
    "\n",
    "# 定义准确率。\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "# 初始化所有的全局变量。\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "\n",
    "# 从这里开始训练我们的模型。\n",
    "# 我们一共训练20000次。\n",
    "for i in range(10):\n",
    "    # 随机在训练集中产生50个数据。\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    # 这一部分用于输出训练过程中的准确率变化。 \n",
    "    if i % 100 == 0:\n",
    "        # 定义训练中的准确率计算，并将数据feed给相应的placeholder，用于训练。\n",
    "        train_accuracy = accuracy.eval(feed_dict = {x:batch[0] , y_:batch[1], keep_prob: 1.0})\n",
    "        # 输出\n",
    "        print(\"step %d,train accuracy %g\"%(i,train_accuracy))\n",
    "    # feed数据，并训练模型。    \n",
    "    train_step.run(feed_dict = {x:batch[0] , y_:batch[1], keep_prob: 0.5})\n",
    "\n",
    "# 将测试集的书记feed给相应的placeholder，然后测试模型的训练结果。\n",
    "print(\"test accuracy %g\"%accuracy.eval(feed_dict = {x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
